{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Reader, Dataset, evaluate, dump\n",
    "from surprise import SVD, SVDpp, KNNBaseline, KNNWithMeans, CoClustering\n",
    "import threading\n",
    "import thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDataFile(data,counter):\n",
    "    df_nan = pd.DataFrame(pd.isnull(data.Rating))\n",
    "    df_nan = df_nan[df_nan['Rating'] == True]\n",
    "    df_nan = df_nan.reset_index()\n",
    "    a = zip(df_nan['index'][1:],df_nan['index'][:-1])\n",
    "    store_df = pd.DataFrame([])\n",
    "    temp_total_array = np.zeros(((0),1))\n",
    "    for i,j in a:\n",
    "        temp_np_array = np.full((i-j,1), counter)\n",
    "        temp_total_array =  np.concatenate((temp_total_array, temp_np_array))\n",
    "        counter = counter+1\n",
    "    remaining =  data.shape[0]- temp_total_array.shape[0]\n",
    "    temp_np_array = np.full((remaining,1), counter)\n",
    "    temp_total_array =  np.concatenate((temp_total_array, temp_np_array))\n",
    "    data['movie_id'] = temp_total_array\n",
    "    data= data.dropna(thresh =3)\n",
    "    return (data,counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pd.DataFrame([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before (0, 0)\n",
      "After (24053764, 4)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./input/combined_data_1.txt\",  names = ['Cust_Id', 'Rating','date'])\n",
    "temp_data = processDataFile(data,1)\n",
    "#print \"Counter\",1\n",
    "print \"Before\",total_data.shape\n",
    "total_data = pd.concat([total_data,temp_data[0]],ignore_index=True)\n",
    "print \"After\",total_data.shape\n",
    "#print mem_usage(data), mem_usage(total_data), \" \", mem_usage(temp_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter 9209\n",
      "Before (102062710, 4)\n",
      "After (129040301, 4)\n",
      "3154.00 MB\n",
      "15084.00 MB   3359.00 MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./input/combined_data_2.txt\",  names = ['Cust_Id', 'Rating','date'])\n",
    "temp_data = processDataFile(data,temp_data[1])\n",
    "print \"Counter\",temp_data[1]\n",
    "print \"Before\",total_data.shape\n",
    "total_data = pd.concat([total_data,temp_data[0]],ignore_index=True)\n",
    "print \"After\",total_data.shape\n",
    "#print mem_usage(data), mem_usage(total_data), \" \", mem_usage(temp_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After (24053764, 4)\n"
     ]
    }
   ],
   "source": [
    "print \"After\",total_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./input/combined_data_3.txt\",  names = ['Cust_Id', 'Rating','date'])\n",
    "temp_data = processDataFile(data,temp_data[1])\n",
    "print \"Counter\",temp_data[1]\n",
    "print \"Before\",total_data.shape\n",
    "total_data = pd.concat([total_data,temp_data[0]],ignore_index=True)\n",
    "print \"After\",total_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24053764, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./input/combined_data_4.txt\",  names = ['Cust_Id', 'Rating','date'])\n",
    "temp_data = processDataFile(data,temp_data[1])\n",
    "print \"Counter\",temp_data[1]\n",
    "print \"Before\",total_data.shape\n",
    "total_data = pd.concat([total_data,temp_data[0]],ignore_index=True)\n",
    "print \"After\",total_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del data\n",
    "sample_df = total_data[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cust_Id</th>\n",
       "      <th>Rating</th>\n",
       "      <th>date</th>\n",
       "      <th>movie_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1488844</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2005-09-06</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>822109</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2005-05-13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>885013</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2005-10-19</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30878</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2005-12-26</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>823519</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2004-05-03</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cust_Id  Rating        date  movie_id\n",
       "0  1488844     3.0  2005-09-06       1.0\n",
       "1   822109     5.0  2005-05-13       1.0\n",
       "2   885013     4.0  2005-10-19       1.0\n",
       "3    30878     4.0  2005-12-26       1.0\n",
       "4   823519     3.0  2004-05-03       1.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.pivot_table(sample_df,index='Cust_Id', columns='movie_id')\n",
    "# temp = pd.pivot_table(sample_df,'movie_id','Cust_Id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create models as a dictionary below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "coclustering = CoClustering()\n",
    "knnbaseline = KNNBaseline()\n",
    "knnwithmeans = KNNWithMeans()\n",
    "svd = SVD()\n",
    "svdpp = SVDpp()\n",
    "# modelList = {\"coclustering\" : coclustering, \"knnbaseline\" : knnbaseline, \"knnwithmeans\" : knnwithmeans, \"svd\" :svd, \"svdpp\" :svdpp}\n",
    "modelList = {\"coclustering\" : coclustering, \"svd\" :svd, \"svdpp\" :svdpp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader()\n",
    "data = Dataset.load_from_df(total_data[['Cust_Id', 'movie_id', 'Rating']].sample(n=500000), reader)\n",
    "data.split(n_folds=3)\n",
    "trainset = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of Probe Data manipulation \n",
    "probe = pd.read_csv(\"./input/probe.txt\",  names = ['Cust_Id'])\n",
    "# temp_data = processDataFile(probe,1)\n",
    "probe_movie = pd.DataFrame([])\n",
    "Cust_Id = []\n",
    "movie_id = []\n",
    "currentMovie =0\n",
    "index = 0\n",
    "\n",
    "for row in probe['Cust_Id']:\n",
    "    if row[len(row)-1] == \":\":\n",
    "        currentMovie = int(row[:-1])\n",
    "        #print currentMovie\n",
    "        continue\n",
    "    Cust_Id.append(row)\n",
    "    movie_id.append(currentMovie)\n",
    "    #print index,row\n",
    "    index +=1\n",
    "probe_movie['Cust_Id'] = Cust_Id\n",
    "probe_movie['movie_id'] = movie_id\n",
    "#print probe_movie[:30]\n",
    "# test = total_data.loc[(total_data.Cust_Id.isin(probe_movie.Cust_Id)) & total_data.movie_id.isin(probe_movie.movie_id)]\n",
    "merged_probe_set = pd.merge(probe_movie,total_data,on=['Cust_Id','movie_id'])\n",
    "#print merged_probe_set[:30]\n",
    "# End of Probe Data manipulation \n",
    "predictions = pd.DataFrame(0, index=np.arange(len(merged_probe_set)), columns=modelList.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateTrainAndTestModel(modelList, testData, predictions):\n",
    "    # Evaluate model\n",
    "    for model in modelList:\n",
    "        print \"Testing model: \", model\n",
    "        try:\n",
    "            evaluate(modelList[model], trainData, measures=['RMSE', 'MAE'])\n",
    "\n",
    "            # Train model\n",
    "            modelList[model].train(trainset)\n",
    "\n",
    "            # Predict Movie ratings and compute error\n",
    "            cust_id = [];\n",
    "            pred = [];\n",
    "            index =1;\n",
    "            mae_sum = 0;\n",
    "            mse_sum = 0;\n",
    "            mse_sum_round = 0;\n",
    "            rmse_round = 0;\n",
    "            rmse = 0;\n",
    "            n = len(merged_probe_set)\n",
    "            for index,row in testData.iterrows():\n",
    "                user = row['Cust_Id']\n",
    "                currentMovie = row['movie_id']\n",
    "                pred = modelList[model].predict(user, currentMovie, r_ui=row['Rating'], verbose=False)\n",
    "                predictions[model][index]= pred.est\n",
    "                diff = row['Rating'] - pred.est\n",
    "                diffRound = row['Rating'] - round(pred.est)\n",
    "                mse_sum += diff**2\n",
    "                mse_sum_round += diffRound**2\n",
    "                #print(diff)\n",
    "            mse = mse_sum/n\n",
    "            rmse = (mse_sum/n)**0.5\n",
    "            rmse_round = (mse_sum_round/n)**0.5\n",
    "\n",
    "            # TODO: Try using model.test\n",
    "            #pred = model.test(testData['Cust_Id'])\n",
    "            #rmse(pred)\n",
    "            #dump.dump('./dump_SVD', pred, model)\n",
    "            #print testData[:20]\n",
    "            print \"MSE: \", mse, \" RMSE: \", rmse , \" RMSE Roundup:  \", rmse_round\n",
    "        except MemoryError:\n",
    "            print \"!!! Memory Error occured !!!\"\n",
    "            print \"Continuing with next model\"\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = Reader()\n",
    "\n",
    "# get just top 100K rows for faster run time\n",
    "data = Dataset.load_from_df(total_data[['Cust_Id', 'movie_id', 'Rating']][:500000], reader)\n",
    "data.split(n_folds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = Reader()\n",
    "data = Dataset.load_from_df(total_data[['Cust_Id', 'movie_id', 'Rating']][:500000], reader)\n",
    "trainset = data.build_full_trainset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runModelsfor():\n",
    "    for i in range(7,20):\n",
    "        print \"Running for i: \",i\n",
    "        reader = Reader()\n",
    "# get just top 100K rows for faster run time\n",
    "        data = Dataset.load_from_df(total_data[['Cust_Id', 'movie_id', 'Rating']][:i*500000], reader)\n",
    "        data.split(n_folds=3)\n",
    "        data = Dataset.load_from_df(total_data[['Cust_Id', 'movie_id', 'Rating']][:i*500000], reader)\n",
    "        trainset = data.build_full_trainset()\n",
    "        \n",
    "        for model in modelList:\n",
    "            try:\n",
    "                print \"Testing model: \", model\n",
    "                res = evaluateTrainAndTestModel(modelList, merged_probe_set, predictions)\n",
    "                print \"For i: MSE: \",res[0],\"RMSE: \",res[1]\n",
    "            except MemoryError:\n",
    "                print \"!!! Memory Error occured !!!\"\n",
    "                print \"Continuing with next model\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "runModelsfor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model:  coclustering\n",
      "Evaluating RMSE, MAE of algorithm CoClustering.\n",
      "\n",
      "------------\n",
      "Fold 1\n",
      "RMSE: 0.9853\n",
      "MAE:  0.7668\n",
      "------------\n",
      "Fold 2\n",
      "RMSE: 0.9824\n",
      "MAE:  0.7640\n",
      "------------\n",
      "Fold 3\n",
      "RMSE: 0.9829\n",
      "MAE:  0.7640\n",
      "------------\n",
      "------------\n",
      "Mean RMSE: 0.9835\n",
      "Mean MAE : 0.7650\n",
      "------------\n",
      "------------\n",
      "MSE:  1.40667508156  RMSE:  1.18603333914  RMSE Roundup:   1.21828917983\n",
      "Testing model:  svd\n",
      "Evaluating RMSE, MAE of algorithm SVD.\n",
      "\n",
      "------------\n",
      "Fold 1\n",
      "RMSE: 0.9544\n",
      "MAE:  0.7478\n",
      "------------\n",
      "Fold 2\n",
      "RMSE: 0.9553\n",
      "MAE:  0.7486\n",
      "------------\n",
      "Fold 3\n",
      "RMSE: 0.9548\n",
      "MAE:  0.7480\n",
      "------------\n",
      "------------\n",
      "Mean RMSE: 0.9548\n",
      "Mean MAE : 0.7481\n",
      "------------\n",
      "------------\n",
      "MSE:  1.1314760371  RMSE:  1.06370862416  RMSE Roundup:   1.10322495319\n",
      "Testing model:  svdpp\n",
      "Evaluating RMSE, MAE of algorithm SVDpp.\n",
      "\n",
      "------------\n",
      "Fold 1\n"
     ]
    }
   ],
   "source": [
    "evaluateTrainAndTestModel(modelList, merged_probe_set, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Data Munging***\n",
    "<br> The data is not available as a csv, instead it is a TXT file. There are four files which contain the customer ids and the rating that they gave to the movies. In each of these files there are rows with a number followed by a colon and  the rows that follow this kind of rows have the customer ids with the rating and the date on which the rating was given.\n",
    "To organize this data we have done the following steps:<br>\n",
    "1. We find all the indices which have movie ids in them and store them in an array.\n",
    "2. We zip this array with itself such that the resulting dataframe contains the value of the next index in the second column.\n",
    "3. We use the resultant dataframe and use the values in the first 2 columns to fill an array with movie ids.\n",
    "4. We append the new array to dataframe that we obtained from the TXT files.\n",
    "5. Finally, we drop those rows which have the movie ids with colons in them.\n",
    "\n",
    "We named this dataframe as total_data.\n",
    "\n",
    "Similarly for the probe set, we read the probe.txt file and store it in a dataframe. Just like we cleaned the movie data set as mentioned above, we have cleaned the probe data as well. The problem with probe data set is that the rows dont have the actual ratings that the movies got. So, we had to find the actual ratings from total_data dataframe by merging the probe dataframe with the total_data dataframe. We named this dataframe as merged_probe_set.<br>\n",
    "\n",
    "In this assignment we have used collaborative learning as our modelling method for unsupervised learning.\n",
    "\n",
    "\n",
    "**What is Collaborative Filtering?**\n",
    "<br>Collaborative Filtering uses preferences of other users to determine the preference of the user in consideration. For example, if person A likes Movie 1,2 and 3, and person B likes movies 1 and 3, then there is a high probability that Person A and B will have affinity for the same kind of movies. So, we can safely assume that Person A and Person B are liked minded and we can use one person's rating for a particular movie to determine the other person's liking for that movie.\n",
    "<br>\n",
    "Formally, Collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc. <a href=\"https://en.wikipedia.org/wiki/Collaborative_filtering\">Source</a><br>\n",
    "\n",
    "**We have used the following clustering techniques with their corresponding MSE and RMSE on the probe set: **<br>\n",
    "\n",
    "1. CoClustering: In Co-Clustering, the users are assigned some clusters and some co-clusters. We calculate the predicion using the sum of the difference of the mean of individual clusters and the difference of the co-clusters.\n",
    "MSE:  1.28873601364  RMSE:  1.13522509382\n",
    "2. SVD: the user-rating matrix of movies after an SVD, will decompose into matrices that represents latent user-user features and item-item features, e.g. same gender user, same age-group of users, action movies etc. and many other latent factors involved in the rating behavior that is not apparent from the user-rating matrix.<a href =\"https://www.quora.com/Whats-the-difference-between-SVD-and-SVD++\">Source</a>\n",
    "3. SVDpp: This is similar to SVD except, it factors in item-item neighborhood and user-user neighborhood models.\n",
    "\n",
    "\n",
    "**Prevention of Over-fitting**\n",
    "\n",
    "Although overfitting is not so common in Unsupervised learning as it is in Supervised learing but there are a certain scenarios where overfitting can occur in Unsupervised learning as well. For example: In the following two scenarios:\n",
    "1. There are N elements and N cluster, and each element belongs to its own cluster.\n",
    "2. On the other hand if there is one big cluster which has all the elements. Then also the error rate would be very high.\n",
    "\n",
    "The module that we used to implement collaborative filtering has a feature in which the training data is divided into folds. The api divides the data into a certain number of folds, lets say 5, and out of these 5 folds it uses 4 folds to train and 1 fold to validate the predictions. This helps us in keeping the overfitting in check.\n",
    "\n",
    "<br>\n",
    "***Individual Contributions:***\n",
    "<br>\n",
    "Our primary motive from this assignment was learning. So, we did not divide the assignment amongst each other instead all three of us individualy and separately did the assignment and merged the best of all three. <br>\n",
    "\n",
    "Overall, we believe that the contribution of all members of the team was equal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
