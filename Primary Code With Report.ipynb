{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Reader, Dataset, evaluate, dump\n",
    "from surprise import SVD, SVDpp, KNNBaseline, KNNWithMeans, CoClustering\n",
    "import threading\n",
    "import thread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processDataFile(data,counter):\n",
    "    df_nan = pd.DataFrame(pd.isnull(data.Rating))\n",
    "    df_nan = df_nan[df_nan['Rating'] == True]\n",
    "    df_nan = df_nan.reset_index()\n",
    "    # print df_na\n",
    "    movie_np = []\n",
    "    movie_id = 1\n",
    "    # print df_nan['index'][1:],df_nan['index'][:-1]\n",
    "    a = zip(df_nan['index'][1:],df_nan['index'][:-1])\n",
    "    store_df = pd.DataFrame([])\n",
    "    temp_total_array = np.zeros(((0),1))\n",
    "    for i,j in a:\n",
    "        temp_np_array = np.full((i-j,1), counter)\n",
    "        temp_total_array =  np.concatenate((temp_total_array, temp_np_array))\n",
    "        counter = counter+1\n",
    "        #print counter\n",
    "    #print temp_total_array.shape\n",
    "    remaining =  data.shape[0]- temp_total_array.shape[0]\n",
    "    temp_np_array = np.full((remaining,1), counter)\n",
    "    temp_total_array =  np.concatenate((temp_total_array, temp_np_array))\n",
    "    data['movie_id'] = temp_total_array\n",
    "    data= data.dropna(thresh =3)\n",
    "    return (data,counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_data = pd.DataFrame([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before (0, 0)\n",
      "After (24053764, 4)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./input/combined_data_1.txt\",  names = ['Cust_Id', 'Rating','date'])\n",
    "temp_data = processDataFile(data,1)\n",
    "#print \"Counter\",1\n",
    "print \"Before\",total_data.shape\n",
    "total_data = pd.concat([total_data,temp_data[0]],ignore_index=True)\n",
    "print \"After\",total_data.shape\n",
    "#print mem_usage(data), mem_usage(total_data), \" \", mem_usage(temp_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter 9209\n",
      "Before (24053764, 4)\n",
      "After (51031355, 4)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./input/combined_data_2.txt\",  names = ['Cust_Id', 'Rating','date'])\n",
    "temp_data = processDataFile(data,temp_data[1])\n",
    "print \"Counter\",temp_data[1]\n",
    "print \"Before\",total_data.shape\n",
    "total_data = pd.concat([total_data,temp_data[0]],ignore_index=True)\n",
    "print \"After\",total_data.shape\n",
    "#print mem_usage(data), mem_usage(total_data), \" \", mem_usage(temp_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After (73632984, 4)\n"
     ]
    }
   ],
   "source": [
    "print \"After\",total_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter 13365\n",
      "Before (51031355, 4)\n",
      "After (73632984, 4)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./input/combined_data_3.txt\",  names = ['Cust_Id', 'Rating','date'])\n",
    "temp_data = processDataFile(data,temp_data[1])\n",
    "print \"Counter\",temp_data[1]\n",
    "print \"Before\",total_data.shape\n",
    "total_data = pd.concat([total_data,temp_data[0]],ignore_index=True)\n",
    "print \"After\",total_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./input/combined_data_4.txt\",  names = ['Cust_Id', 'Rating','date'])\n",
    "temp_data = processDataFile(data,temp_data[1])\n",
    "print \"Counter\",temp_data[1]\n",
    "print \"Before\",total_data.shape\n",
    "total_data = pd.concat([total_data,temp_data[0]],ignore_index=True)\n",
    "print \"After\",total_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del data\n",
    "sample_df = total_data[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cust_Id</th>\n",
       "      <th>Rating</th>\n",
       "      <th>date</th>\n",
       "      <th>movie_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1488844</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2005-09-06</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>822109</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2005-05-13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>885013</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2005-10-19</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30878</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2005-12-26</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>823519</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2004-05-03</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cust_Id  Rating        date  movie_id\n",
       "0  1488844     3.0  2005-09-06       1.0\n",
       "1   822109     5.0  2005-05-13       1.0\n",
       "2   885013     4.0  2005-10-19       1.0\n",
       "3    30878     4.0  2005-12-26       1.0\n",
       "4   823519     3.0  2004-05-03       1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = pd.pivot_table(sample_df,index='Cust_Id', columns='movie_id')\n",
    "# temp = pd.pivot_table(sample_df,'movie_id','Cust_Id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create models as a dictionary below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coclustering = CoClustering()\n",
    "knnbaseline = KNNBaseline()\n",
    "knnwithmeans = KNNWithMeans()\n",
    "svd = SVD()\n",
    "svdpp = SVDpp()\n",
    "modelList = {\"coclustering\" : coclustering, \"knnbaseline\" : knnbaseline, \"knnwithmeans\" : knnwithmeans, \"svd\" :svd, \"svdpp\" :svdpp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = Reader()\n",
    "                                                                                                                                                                                                                                                                                                                                                \n",
    "# get just top 100K rows for faster run time\n",
    "trainData = Dataset.load_from_df(total_data[['Cust_Id', 'movie_id', 'Rating']].sample(n=10000000), reader)\n",
    "#data.split(n_folds=3)\n",
    "#trainset = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData.split(n_folds=3)\n",
    "trainset = trainData.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start of Probe Data manipulation \n",
    "probe = pd.read_csv(\"./input/probe.txt\",  names = ['Cust_Id'])\n",
    "# temp_data = processDataFile(probe,1)\n",
    "probe_movie = pd.DataFrame([])\n",
    "Cust_Id = []\n",
    "movie_id = []\n",
    "currentMovie =0\n",
    "index = 0\n",
    "\n",
    "for row in probe['Cust_Id']:\n",
    "    if row[len(row)-1] == \":\":\n",
    "        currentMovie = int(row[:-1])\n",
    "        #print currentMovie\n",
    "        continue\n",
    "    Cust_Id.append(row)\n",
    "    movie_id.append(currentMovie)\n",
    "    #print index,row\n",
    "    index +=1\n",
    "probe_movie['Cust_Id'] = Cust_Id\n",
    "probe_movie['movie_id'] = movie_id\n",
    "#print probe_movie[:30]\n",
    "# test = total_data.loc[(total_data.Cust_Id.isin(probe_movie.Cust_Id)) & total_data.movie_id.isin(probe_movie.movie_id)]\n",
    "merged_probe_set = pd.merge(probe_movie,total_data,on=['Cust_Id','movie_id'])\n",
    "#print merged_probe_set[:30]\n",
    "# End of Probe Data manipulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1425333\n"
     ]
    }
   ],
   "source": [
    "print len(probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateTrainAndTestModel(modelList, testData, predictions):\n",
    "    # Evaluate model\n",
    "    for model in modelList:\n",
    "        print \"Testing model: \", model\n",
    "        try:\n",
    "            evaluate(modelList[model], trainData, measures=['RMSE', 'MAE'])\n",
    "\n",
    "            # Train model\n",
    "            modelList[model].train(trainset)\n",
    "\n",
    "            # Predict Movie ratings and compute error\n",
    "            cust_id = [];\n",
    "            pred = [];\n",
    "            index =1;\n",
    "            mae_sum = 0;\n",
    "            mse_sum = 0;\n",
    "            mse_sum_round = 0;\n",
    "            rmse_round = 0;\n",
    "            rmse = 0;\n",
    "            n = len(merged_probe_set)\n",
    "            for index,row in testData.iterrows():\n",
    "                user = row['Cust_Id']\n",
    "                currentMovie = row['movie_id']\n",
    "                pred = modelList[model].predict(user, currentMovie, r_ui=row['Rating'], verbose=False)\n",
    "                predictions[model][index]= pred.est\n",
    "                diff = row['Rating'] - pred.est\n",
    "                diffRound = row['Rating'] - round(pred.est)\n",
    "                mse_sum += diff**2\n",
    "                mse_sum_round += diffRound**2\n",
    "                #print(diff)\n",
    "            mse = mse_sum/n\n",
    "            rmse = (mse_sum/n)**0.5\n",
    "            rmse_round = (mse_sum_round/n)**0.5\n",
    "\n",
    "            # TODO: Try using model.test\n",
    "            #pred = model.test(testData['Cust_Id'])\n",
    "            #rmse(pred)\n",
    "            #dump.dump('./dump_SVD', pred, model)\n",
    "            #print testData[:20]\n",
    "            print \"MSE: \", mse, \" RMSE: \", rmse , \" RMSE Roundup:  \", rmse_round\n",
    "        except MemoryError:\n",
    "            print \"!!! Memory Error occured !!!\"\n",
    "            print \"Continuing with next model\"\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model:  coclustering\n",
      "Evaluating RMSE, MAE of algorithm CoClustering.\n",
      "\n",
      "------------\n",
      "Fold 1\n",
      "RMSE: 0.9854\n",
      "MAE:  0.7670\n",
      "------------\n",
      "Fold 2\n",
      "RMSE: 0.9840\n",
      "MAE:  0.7657\n",
      "------------\n",
      "Fold 3\n",
      "RMSE: 0.9839\n",
      "MAE:  0.7658\n",
      "------------\n",
      "------------\n",
      "Mean RMSE: 0.9845\n",
      "Mean MAE : 0.7662\n",
      "------------\n",
      "------------\n",
      "    Cust_Id movie_id  Rating        date\n",
      "0     30878        1     4.0  2005-12-26\n",
      "1   2647871        1     4.0  2005-12-30\n",
      "2   1283744        1     3.0  2004-04-17\n",
      "3   2488120        1     5.0  2005-09-20\n",
      "4    317050        1     5.0  2005-11-15\n",
      "5   1904905        1     4.0  2005-05-13\n",
      "6   1989766        1     4.0  2005-07-08\n",
      "7     14756        1     4.0  2005-12-27\n",
      "8   1027056        1     3.0  2005-12-03\n",
      "9   1149588        1     4.0  2005-12-13\n",
      "10  1394012        1     5.0  2005-12-19\n",
      "11  1406595        1     4.0  2005-08-27\n",
      "12  2529547        1     5.0  2005-11-18\n",
      "13  1682104        1     4.0  2005-08-30\n",
      "14  2625019        1     3.0  2005-09-12\n",
      "15  2603381        1     5.0  2005-11-29\n",
      "16  1774623        1     4.0  2005-11-23\n",
      "17   470861        1     5.0  2004-06-28\n",
      "18   712610        1     4.0  2005-04-27\n",
      "19  1772839        1     5.0  2005-09-19\n",
      "MSE:  1.13283932008  RMSE:  1.06434924723  RMSE Roundup:   1.10445979131\n",
      "Evaluating RMSE, MAE of algorithm KNNBaseline.\n",
      "\n",
      "------------\n",
      "Fold 1\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "!!! Memory Error occured !!!\n",
      "Continuing with next model\n",
      "Testing model:  knnbaseline\n",
      "Evaluating RMSE, MAE of algorithm CoClustering.\n",
      "\n",
      "------------\n",
      "Fold 1\n",
      "RMSE: 0.9825\n",
      "MAE:  0.7644\n",
      "------------\n",
      "Fold 2\n",
      "RMSE: 0.9848\n",
      "MAE:  0.7658\n",
      "------------\n",
      "Fold 3\n",
      "RMSE: 0.9853\n",
      "MAE:  0.7665\n",
      "------------\n",
      "------------\n",
      "Mean RMSE: 0.9842\n",
      "Mean MAE : 0.7656\n",
      "------------\n",
      "------------\n",
      "    Cust_Id movie_id  Rating        date\n",
      "0     30878        1     4.0  2005-12-26\n",
      "1   2647871        1     4.0  2005-12-30\n",
      "2   1283744        1     3.0  2004-04-17\n",
      "3   2488120        1     5.0  2005-09-20\n",
      "4    317050        1     5.0  2005-11-15\n",
      "5   1904905        1     4.0  2005-05-13\n",
      "6   1989766        1     4.0  2005-07-08\n",
      "7     14756        1     4.0  2005-12-27\n",
      "8   1027056        1     3.0  2005-12-03\n",
      "9   1149588        1     4.0  2005-12-13\n",
      "10  1394012        1     5.0  2005-12-19\n",
      "11  1406595        1     4.0  2005-08-27\n",
      "12  2529547        1     5.0  2005-11-18\n",
      "13  1682104        1     4.0  2005-08-30\n",
      "14  2625019        1     3.0  2005-09-12\n",
      "15  2603381        1     5.0  2005-11-29\n",
      "16  1774623        1     4.0  2005-11-23\n",
      "17   470861        1     5.0  2004-06-28\n",
      "18   712610        1     4.0  2005-04-27\n",
      "19  1772839        1     5.0  2005-09-19\n",
      "MSE:  1.13288291451  RMSE:  1.06436972641  RMSE Roundup:   1.10430215339\n",
      "Evaluating RMSE, MAE of algorithm KNNBaseline.\n",
      "\n",
      "------------\n",
      "Fold 1\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "!!! Memory Error occured !!!\n",
      "Continuing with next model\n",
      "Testing model:  svd\n",
      "Evaluating RMSE, MAE of algorithm CoClustering.\n",
      "\n",
      "------------\n",
      "Fold 1\n",
      "RMSE: 0.9851\n",
      "MAE:  0.7667\n",
      "------------\n",
      "Fold 2\n",
      "RMSE: 0.9841\n",
      "MAE:  0.7655\n",
      "------------\n",
      "Fold 3\n",
      "RMSE: 0.9840\n",
      "MAE:  0.7659\n",
      "------------\n",
      "------------\n",
      "Mean RMSE: 0.9844\n",
      "Mean MAE : 0.7660\n",
      "------------\n",
      "------------\n",
      "    Cust_Id movie_id  Rating        date\n",
      "0     30878        1     4.0  2005-12-26\n",
      "1   2647871        1     4.0  2005-12-30\n",
      "2   1283744        1     3.0  2004-04-17\n",
      "3   2488120        1     5.0  2005-09-20\n",
      "4    317050        1     5.0  2005-11-15\n",
      "5   1904905        1     4.0  2005-05-13\n",
      "6   1989766        1     4.0  2005-07-08\n",
      "7     14756        1     4.0  2005-12-27\n",
      "8   1027056        1     3.0  2005-12-03\n",
      "9   1149588        1     4.0  2005-12-13\n",
      "10  1394012        1     5.0  2005-12-19\n",
      "11  1406595        1     4.0  2005-08-27\n",
      "12  2529547        1     5.0  2005-11-18\n",
      "13  1682104        1     4.0  2005-08-30\n",
      "14  2625019        1     3.0  2005-09-12\n",
      "15  2603381        1     5.0  2005-11-29\n",
      "16  1774623        1     4.0  2005-11-23\n",
      "17   470861        1     5.0  2004-06-28\n",
      "18   712610        1     4.0  2005-04-27\n",
      "19  1772839        1     5.0  2005-09-19\n",
      "MSE:  1.13262957823  RMSE:  1.06425071211  RMSE Roundup:   1.10447221501\n",
      "Evaluating RMSE, MAE of algorithm KNNBaseline.\n",
      "\n",
      "------------\n",
      "Fold 1\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "!!! Memory Error occured !!!\n",
      "Continuing with next model\n",
      "Testing model:  knnwithmeans\n",
      "Evaluating RMSE, MAE of algorithm CoClustering.\n",
      "\n",
      "------------\n",
      "Fold 1\n",
      "RMSE: 0.9853\n",
      "MAE:  0.7668\n",
      "------------\n",
      "Fold 2\n",
      "RMSE: 0.9849\n",
      "MAE:  0.7659\n",
      "------------\n",
      "Fold 3\n",
      "RMSE: 0.9850\n",
      "MAE:  0.7664\n",
      "------------\n",
      "------------\n",
      "Mean RMSE: 0.9851\n",
      "Mean MAE : 0.7664\n",
      "------------\n",
      "------------\n",
      "    Cust_Id movie_id  Rating        date\n",
      "0     30878        1     4.0  2005-12-26\n",
      "1   2647871        1     4.0  2005-12-30\n",
      "2   1283744        1     3.0  2004-04-17\n",
      "3   2488120        1     5.0  2005-09-20\n",
      "4    317050        1     5.0  2005-11-15\n",
      "5   1904905        1     4.0  2005-05-13\n",
      "6   1989766        1     4.0  2005-07-08\n",
      "7     14756        1     4.0  2005-12-27\n",
      "8   1027056        1     3.0  2005-12-03\n",
      "9   1149588        1     4.0  2005-12-13\n",
      "10  1394012        1     5.0  2005-12-19\n",
      "11  1406595        1     4.0  2005-08-27\n",
      "12  2529547        1     5.0  2005-11-18\n",
      "13  1682104        1     4.0  2005-08-30\n",
      "14  2625019        1     3.0  2005-09-12\n",
      "15  2603381        1     5.0  2005-11-29\n",
      "16  1774623        1     4.0  2005-11-23\n",
      "17   470861        1     5.0  2004-06-28\n",
      "18   712610        1     4.0  2005-04-27\n",
      "19  1772839        1     5.0  2005-09-19\n",
      "MSE:  1.12484945408  RMSE:  1.06058920138  RMSE Roundup:   1.10051284857\n",
      "Evaluating RMSE, MAE of algorithm KNNBaseline.\n",
      "\n",
      "------------\n",
      "Fold 1\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "!!! Memory Error occured !!!\n",
      "Continuing with next model\n",
      "Testing model:  svdpp\n",
      "Evaluating RMSE, MAE of algorithm CoClustering.\n",
      "\n",
      "------------\n",
      "Fold 1\n",
      "RMSE: 0.9851\n",
      "MAE:  0.7666\n",
      "------------\n",
      "Fold 2\n",
      "RMSE: 0.9852\n",
      "MAE:  0.7665\n",
      "------------\n",
      "Fold 3\n",
      "RMSE: 0.9852\n",
      "MAE:  0.7665\n",
      "------------\n",
      "------------\n",
      "Mean RMSE: 0.9852\n",
      "Mean MAE : 0.7665\n",
      "------------\n",
      "------------\n",
      "    Cust_Id movie_id  Rating        date\n",
      "0     30878        1     4.0  2005-12-26\n",
      "1   2647871        1     4.0  2005-12-30\n",
      "2   1283744        1     3.0  2004-04-17\n",
      "3   2488120        1     5.0  2005-09-20\n",
      "4    317050        1     5.0  2005-11-15\n",
      "5   1904905        1     4.0  2005-05-13\n",
      "6   1989766        1     4.0  2005-07-08\n",
      "7     14756        1     4.0  2005-12-27\n",
      "8   1027056        1     3.0  2005-12-03\n",
      "9   1149588        1     4.0  2005-12-13\n",
      "10  1394012        1     5.0  2005-12-19\n",
      "11  1406595        1     4.0  2005-08-27\n",
      "12  2529547        1     5.0  2005-11-18\n",
      "13  1682104        1     4.0  2005-08-30\n",
      "14  2625019        1     3.0  2005-09-12\n",
      "15  2603381        1     5.0  2005-11-29\n",
      "16  1774623        1     4.0  2005-11-23\n",
      "17   470861        1     5.0  2004-06-28\n",
      "18   712610        1     4.0  2005-04-27\n",
      "19  1772839        1     5.0  2005-09-19\n",
      "MSE:  1.13299799933  RMSE:  1.06442378747  RMSE Roundup:   1.10410954056\n",
      "Evaluating RMSE, MAE of algorithm KNNBaseline.\n",
      "\n",
      "------------\n",
      "Fold 1\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "!!! Memory Error occured !!!\n",
      "Continuing with next model\n"
     ]
    }
   ],
   "source": [
    "predictions = pd.DataFrame(0, index=np.arange(len(merged_probe_set)), columns=modelList.keys())\n",
    "evaluateTrainAndTestModel(modelList, merged_probe_set, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        coclustering  knnbaseline  svd  knnwithmeans  svdpp\n",
      "0                  3            0    0             0      0\n",
      "1                  3            0    0             0      0\n",
      "2                  3            0    0             0      0\n",
      "3                  4            0    0             0      0\n",
      "4                  4            0    0             0      0\n",
      "5                  4            0    0             0      0\n",
      "6                  3            0    0             0      0\n",
      "7                  3            0    0             0      0\n",
      "8                  2            0    0             0      0\n",
      "9                  3            0    0             0      0\n",
      "10                 3            0    0             0      0\n",
      "11                 3            0    0             0      0\n",
      "12                 3            0    0             0      0\n",
      "13                 4            0    0             0      0\n",
      "14                 3            0    0             0      0\n",
      "15                 4            0    0             0      0\n",
      "16                 3            0    0             0      0\n",
      "17                 4            0    0             0      0\n",
      "18                 3            0    0             0      0\n",
      "19                 4            0    0             0      0\n",
      "20                 3            0    0             0      0\n",
      "21                 4            0    0             0      0\n",
      "22                 3            0    0             0      0\n",
      "23                 2            0    0             0      0\n",
      "24                 2            0    0             0      0\n",
      "25                 3            0    0             0      0\n",
      "26                 3            0    0             0      0\n",
      "27                 1            0    0             0      0\n",
      "28                 4            0    0             0      0\n",
      "29                 4            0    0             0      0\n",
      "...              ...          ...  ...           ...    ...\n",
      "582994             4            0    0             0      0\n",
      "582995             5            0    0             0      0\n",
      "582996             3            0    0             0      0\n",
      "582997             2            0    0             0      0\n",
      "582998             4            0    0             0      0\n",
      "582999             4            0    0             0      0\n",
      "583000             2            0    0             0      0\n",
      "583001             4            0    0             0      0\n",
      "583002             3            0    0             0      0\n",
      "583003             4            0    0             0      0\n",
      "583004             4            0    0             0      0\n",
      "583005             3            0    0             0      0\n",
      "583006             3            0    0             0      0\n",
      "583007             2            0    0             0      0\n",
      "583008             2            0    0             0      0\n",
      "583009             2            0    0             0      0\n",
      "583010             2            0    0             0      0\n",
      "583011             2            0    0             0      0\n",
      "583012             1            0    0             0      0\n",
      "583013             2            0    0             0      0\n",
      "583014             2            0    0             0      0\n",
      "583015             3            0    0             0      0\n",
      "583016             3            0    0             0      0\n",
      "583017             2            0    0             0      0\n",
      "583018             2            0    0             0      0\n",
      "583019             1            0    0             0      0\n",
      "583020             2            0    0             0      0\n",
      "583021             3            0    0             0      0\n",
      "583022             2            0    0             0      0\n",
      "583023             3            0    0             0      0\n",
      "\n",
      "[583024 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Data Munging***\n",
    "<br> The data is not available as a csv, instead it is a TXT file. There are four files which contain the customer ids and the rating that they gave to the movies. In each of these files there are rows with a number followed by a colon and  the rows that follow this kind of rows have the customer ids with the rating and the date on which the rating was given.\n",
    "To organize this data we have done the following steps:<br>\n",
    "1. We find all the indices which have movie ids in them and store them in an array.\n",
    "2. We zip this array with itself such that the resulting dataframe contains the value of the next index in the second column.\n",
    "3. We use the resultant dataframe and use the values in the first 2 columns to fill an array with movie ids.\n",
    "4. We append the new array to dataframe that we obtained from the TXT files.\n",
    "5. Finally, we drop those rows which have the movie ids with colons in them.\n",
    "\n",
    "We named this dataframe as total_data.\n",
    "\n",
    "Similarly for the probe set, we read the probe.txt file and store it in a dataframe. Just like we cleaned the movie data set as mentioned above, we have cleaned the probe data as well. The problem with probe data set is that the rows dont have the actual ratings that the movies got. So, we had to find the actual ratings from total_data dataframe by merging the probe dataframe with the total_data dataframe. We named this dataframe as merged_probe_set.<br>\n",
    "\n",
    "In this assignment we have used collaborative learning as our modelling method for unsupervised learning.\n",
    "\n",
    "\n",
    "**What is Collaborative Filtering?**\n",
    "<br>Collaborative Filtering uses preferences of other users to determine the preference of the user in consideration. For example, if person A likes Movie 1,2 and 3, and person B likes movies 1 and 3, then there is a high probability that Person A and B will have affinity for the same kind of movies. So, we can safely assume that Person A and Person B are liked minded and we can use one person's rating for a particular movie to determine the other person's liking for that movie.\n",
    "<br>\n",
    "Formally, Collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc. <a href=\"https://en.wikipedia.org/wiki/Collaborative_filtering\">Source</a><br>\n",
    "\n",
    "**We have used the following clustering techniques with their corresponding MSE and RMSE on the probe set: **<br>\n",
    "\n",
    "1. CoClustering: In Co-Clustering, the users are assigned some clusters and some co-clusters. We calculate the predicion using the sum of the difference of the mean of individual clusters and the difference of the co-clusters.\n",
    "MSE:  1.28873601364  RMSE:  1.13522509382\n",
    "2. SVD: the user-rating matrix of movies after an SVD, will decompose into matrices that represents latent user-user features and item-item features, e.g. same gender user, same age-group of users, action movies etc. and many other latent factors involved in the rating behavior that is not apparent from the user-rating matrix.<a href =\"https://www.quora.com/Whats-the-difference-between-SVD-and-SVD++\">Source</a>\n",
    "3. SVDpp: This is similar to SVD except, it factors in item-item neighborhood and user-user neighborhood models.\n",
    "\n",
    "**Other Methods Used**\n",
    "1. The basic model which we built for this dataset was Linear Regression. The accuracy for prediction of the movie ratings in the probe dataset was sufficiently high, as close as 1.1301 over the entire dataset. However, linear regression is not recommended for such a dataset as this is specifically designed for prediction of ratings either by pattern finding or clustering.\n",
    "\n",
    "2. We tried converting the given problem into a supervised learning one. We divided the ratings into 5 classes namely 1 to 5, and then tried predicting the classes for a small dataset. The data being skewed and unsuitable for classification due to lack of features, it almost always failed.\n",
    "\n",
    "**Prevention of Over-fitting**\n",
    "\n",
    "Although overfitting is not so common in Unsupervised learning as it is in Supervised learing but there are a certain scenarios where overfitting can occur in Unsupervised learning as well. For example: In the following two scenarios:\n",
    "1. There are N elements and N cluster, and each element belongs to its own cluster.\n",
    "2. On the other hand if there is one big cluster which has all the elements. Then also the error rate would be very high.\n",
    "\n",
    "The module that we used to implement collaborative filtering has a feature in which the training data is divided into folds. The api divides the data into a certain number of folds, lets say 5, and out of these 5 folds it uses 4 folds to train and 1 fold to validate the predictions. This helps us in keeping the overfitting in check.\n",
    "\n",
    "**Hyper-parameter Tuning**\n",
    "We implemented parameter tuning for SVD and SVDpp models to try the performance change. We used the GridSearch class of Scikit_Surprise for this purpose. We tried setting different n_epoch values for these models and figured out that the accuracy was better when this value is close to 5.\n",
    "\n",
    "**Results**\n",
    "Following are the MSE and RMSE for the Models that we ran:\n",
    "\n",
    "1. CoClustering: MSE:  1.13283932008  RMSE:  1.06434924723  RMSE Roundup:   1.10445979131\n",
    "2. KNNBaseline: Memory Error\n",
    "3. SVD: MSE:  1.1314760371  RMSE:  1.06370862416  RMSE Roundup:   1.10322495319\n",
    "4. SVDpp: MSE:  1.0972537568 RMSE: 1.04749880993\n",
    "5. Linear Regression: RMSE: 1.130103297749197\n",
    "\n",
    "<br>\n",
    "***Individual Contributions:***\n",
    "<br>\n",
    "Ishupreet Singh 111424300  <br> \n",
    "Bhushan Sonawane 111511679 <br>\n",
    "Nishant Borude 111447198 <br>\n",
    "Our primary motive from this assignment was learning. So, we did not divide the assignment amongst each other instead all three of us individualy and separately did the assignment and merged the best of all three. <br>\n",
    "\n",
    "Overall, we believe that the contribution of all members of the team was equal.\n",
    "\n",
    "***Issues Faced***\n",
    "Due to lack of resources most of the highly accurate prediction models such as KNNBaseline, KNNWithMeans would crash with memory error even on 16GB RAM. We are currently implementing our solution for memory intensive models on Google Cloud Platform which might take some time for coming up with an updated report.\n",
    "\n",
    "***References***\n",
    "https://www.kaggle.com/laowingkin/netflix-movie-recommendation\n",
    "Followed the kernel provided by D. Lao on the Netflix Data Challenge page for data preprocessing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
